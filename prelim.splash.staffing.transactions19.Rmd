---
title: "Splash Staffing Analysis - Dollars & Transactions"
author: "Allocate Analytics"
date: "February 23, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#SPLASH
# next step is try to look for trends
# are there staffing changes to be made or will it always be same #

library(dplyr)
library(ggplot2)
library(sugrrants)
library(lubridate)
library(knitr)
library(scales)
library(gridExtra)
library(fpp2)
#library(prophet)

#data.clean <- read.csv("data/mutated.data1417.csv", stringsAsFactors = FALSE)

#####
import.jh.sql.connect <- function(db.name.in.sql.server){
  library(odbc)
  library(DBI)
  
  con <- DBI::dbConnect(odbc::odbc(),
                        Driver = "SQL Server",
                        Server = "LAPTOP-JHRVTF04\\SQLEXPRESS",
                        Database = db.name.in.sql.server,
                        Trusted_Connection = "True")
  con
}

import.jh.detailed.sales <- function(){
  joined.df <- DBI::dbGetQuery(con,'
                                SELECT TransactionEntry.ItemID, TransactionEntry.Price, 
                                 TransactionEntry.TransactionTime, TransactionEntry.Quantity, 
                                  TransactionEntry.TransactionNumber, Item.Description, 
                                 Department.Name AS "Department", Category.Name AS "Category", 
                                 SupplierName AS "Supplier", Cashier.Name AS "CashierName", 
                                 Register.Number AS "Register", Register.Description AS "RegLocation"

                                 FROM TransactionEntry
                                 
                                 JOIN Item ON TransactionEntry.ItemID = Item.ID
                                 JOIN Department ON Item.DepartmentID = Department.ID
                                 JOIN Category ON Item.CategoryID = Category.ID
                                 JOIN Supplier ON Item.SupplierID = Supplier.ID
                                 JOIN [Transaction] ON TransactionEntry.TransactionNumber = [Transaction].TransactionNumber
                                 JOIN Batch ON [Transaction].BatchNumber = Batch.BatchNumber
                                 JOIN Register ON Batch.RegisterID = Register.ID
                                 JOIN Cashier ON [Transaction].CashierID = Cashier.ID')
  joined.df
  }

clean.jh.detailed.sales <- function(sql.df){
  library(dplyr)
  library(lubridate)
    
    sql.df <- sql.df %>%
      rename(Sold.Price = Price, Cashier = CashierName, Date.Sold = TransactionTime,
             Qty.Sold = Quantity, Transaction = TransactionNumber) %>%
      mutate(Total.Sales = Qty.Sold*Sold.Price) %>%
      select(Department, Category, Supplier, ItemID, Description, Qty.Sold, Sold.Price, 
             Total.Sales, Transaction, Date.Sold, Register, Cashier)
    
    sql.df$Date.Sold <- ymd_hms(sql.df$Date.Sold)
    sql.df$Year <- year(sql.df$Date.Sold)
    sql.df$Month <- month(sql.df$Date.Sold)
    sql.df$Department <- trimws(sql.df$Department)
    sql.df$Category <- trimws(sql.df$Category)
    sql.df$Supplier <- trimws(sql.df$Supplier)
    sql.df$Item <- trimws(sql.df$Item)
    #print(head(sql.df))
    sql.df$Dept.By.Year <- paste(sql.df$Department, sql.df$Year)
    sql.df$Categ.By.Year <- paste(sql.df$Category, sql.df$Year)
    sql.df
}
if(file.exists(paste0("data/sales.data.cache", Sys.Date(), ".csv"))){
  data.clean <- read.csv(paste0("data/sales.data.cache", Sys.Date(), ".csv"), stringsAsFactor = FALSE)
} else {
con <- import.jh.sql.connect("WHALESTALE")

new.df <- import.jh.detailed.sales() # uses con but not as argument

data.clean <- clean.jh.detailed.sales(new.df)

write.csv(data.clean, paste0("data/sales.data.cache", Sys.Date(), ".csv"), row.names = FALSE)
}
#####
#data4years <- read.csv("mutated.data1417.csv", stringsAsFactors = FALSE)
data4years <- data.clean
data4years <- filter(data4years, Register == "9")

data4years.wide <- data4years
data4years.wide$Date.Sold <- as_datetime(data4years.wide$Date.Sold)
data4years.wide$Date.Sold.Round <- floor_date(data4years.wide$Date.Sold, unit = "hour")
data4years.wide$Date.Sold <- as_date(data4years.wide$Date.Sold)

data18 <- filter(data4years.wide, Year == 2018)

data18 <- data18[!duplicated(data18$Transaction),] ## makes only one row per transaction

data18.agg <- aggregate(Total.Sales ~ Date.Sold.Round + Date.Sold, data18, length) ### switched from sum to length
data18.agg <- data18.agg %>%
  mutate("Hour" = as.integer(hour(Date.Sold.Round))) %>%
  frame_calendar(x = Hour, y = Total.Sales, date = Date.Sold, calendar = "monthly")
data18.plot <- data18.agg %>%
  ggplot(aes(x = .Hour, y = .Total.Sales, group = Date.Sold)) + geom_line()

```

## Headlines:

* Predicting when it will be busy is harder at Splash than at Whale's Tale.  It tends to be more random, but there are still some times where more transactions per hour or more dollars per hour are more likely.

* To answer the primary question of when Splash is most likely to be busy, the answer is 2-3pm and sometimes 1-3pm.  This would be true from May to September.  In the other months of the year, there's not a clear trend for when it would be busier.

* The next question is whether it's worth it to add staff to capture the extra sales.  A rough calculation of adding another person for 4 hours every day for 5 months project that the added revenue might be $5,000-$9,000 but added compensation might be $7000-$9000.  If there were a way for the shifts to overlap from 1-3 or some similar timeframe, that's most likely to have a positive impact.

* If it were my choice I'd first make sure every worker was as good as they could be serving multiple customers simultaneously.  Next, I'd try to have the shifts overlap by two hours around 1-3, but also have some other productive task the second person could do besides talking to customers when there aren't multiple customers in Splash. 


#### Question To Answer: How can sales data help inform staffing levels through the day through each month of the year?

#### Approach To Answer Question:

1. Visualize 2018 sales by hour for each day of the year in order to see trends (also visualize 2017 to confirm it's fairly similar to 2018)

2. Group together similar days into a number of "sales profiles" for consistency and simplicity.

3. Plot the range of sales that happen through the day for each of the profiles.

## Sales By Hour For Each Day of 2018

```{r calendar.18.plot, echo=FALSE}
#print(unique(data4years$Register))
prettify(data18.plot)
```

## Comments and Trends

For Splash I divided the year into four profiles, as shown in the colors below.

## Sales By Hour For Each Day of 2017

```{r calendar.17.plot, echo=FALSE}
data17 <- filter(data4years.wide, Year == 2017)

data17 <- data17[!duplicated(data17$Transaction),] ## makes only one row per transaction

data17.agg <- aggregate(Total.Sales ~ Date.Sold.Round + Date.Sold, data17, length) # changed from sum
data17.agg <- data17.agg %>%
  mutate("Hour" = as.integer(hour(Date.Sold.Round))) %>%
  frame_calendar(x = Hour, y = Total.Sales, date = Date.Sold, calendar = "monthly")
data17.plot <- data17.agg %>%
  ggplot(aes(x = .Hour, y = .Total.Sales, group = Date.Sold)) + geom_line()
prettify(data17.plot)

```


```{r off.peak.minimum, echo=FALSE}

data2years.wide <- filter(data4years.wide, Year %in% c(2017, 2018))

#data2years$Date.Sold <- as_date(data2years$Date.Sold)

data2years.wide <- data2years.wide %>%
  mutate("Hour" = hour(Date.Sold.Round)) %>%
  mutate("Weekday" = wday((Date.Sold.Round)))
    
# filter for off.peak.minimum ##
#df1 <- data2years.wide %>%
#  filter(Date.Sold >= "2016-01-01", Date.Sold <= "2016-01-31")
#df2 <- data2years.wide %>%
#  filter(Date.Sold >= "2016-02-01", Date.Sold <= "2016-04-07", Weekday %in% c(1, 2, 3, 4, 5, 7))

#df.all <- rbind(df1, df2)

staffing.day.plot.dots <- function(df, nickname.of.time.range){
  
  library(lubridate)
  library(dplyr)
  library(ggplot2)
  
  df.agg <- aggregate(Total.Sales ~ Date.Sold.Round + Hour + Weekday, df, sum)
  conf.df <- data.frame()
  
  for(i in 9:23){
    #print(i)
    i.hour.vec <- df.agg %>%
      filter(Hour == i) %>%
      select(Total.Sales)
    #i.hour.vec <- as.double(i.hour.vec)
    #print(head(i.hour.vec))
    #i.estimate.95 <- t.test(i.hour.vec, conf.level = 0.95)$estimate
    #i.conf.int.95 <- t.test(i.hour.vec, conf.level = 0.95)$conf.int
    #i.conf.int.75 <- t.test(i.hour.vec, conf.level = 0.75)$conf.int
    #i.conf.int.50 <- t.test(i.hour.vec, conf.level = 0.50)$conf.int
    #three.conf.int <- c(i, i.estimate.95, i.conf.int.95, i.conf.int.75, i.conf.int.50)
    quantiles <- quantile(as.numeric(unlist(i.hour.vec)), probs = c(.5, .025, .975, .125, .875, .25, .75))
    #print(class(quantiles))
    
    quantiles.hour <- c(i, quantiles)
    #print(class(quantiles.hour))
    conf.df <- rbind(conf.df, quantiles.hour)
    
    #i.estimate.95 <- quantie(i.hour.vec, .5)
    #i.conf.int.95.high <- 
    #quantiles.hour <- c(i, quantile(i.hour.vec, c(.5, .025, .975, .125, .875, .25, .75)))
  }
  colnames(conf.df) <- c("hour", "estimate", "low95", "high95", "low75", "high75", "low50", "high50")
  
  # add column corresponding to # of staff needed based on $400 -> 2, $600 -> 3, etc
  conf.df$staff.estimate <- (round(conf.df$high50/200))
  
  lbls = paste0(as.character(c(seq(9, 12, 1), seq(1, 11, 1))), c(rep("am", 3), rep("pm", 11)))
  brks <- seq(9,23, 1)
  
  lbls.y = paste0("$", seq(0, 2000, 200))
  brks.y = seq(0, 2000, 200)
  brks.y2 = seq(0,8,1)
  
  #removing dots because not based on anything for Splash
  day.plot <- ggplot(data = conf.df, aes(x=hour)) +
    geom_ribbon(aes(ymin=low95, ymax=high95), fill="#0571b0", alpha = .75)+
    geom_ribbon(aes(ymin=low75, ymax=high75), fill="#92c5de", alpha = .75)+
    geom_ribbon(aes(ymin=low50, ymax=high50), fill="#d1e5f0", alpha = .75)+
    geom_ribbon(aes(ymin=estimate-5, ymax=estimate+5), fill="#ef8a62") #+
    #geom_point(aes(y=staff.estimate*200), color="#FFFF33", size = 10) +
    #geom_text(aes(y = staff.estimate*200, label = staff.estimate), color = "black", size = 5, fontface = "bold")
  day.plot <- day.plot + scale_x_continuous(labels = lbls, breaks = brks) + 
    scale_y_continuous(sec.axis = sec_axis(~./200, name = "Estimate of Hourly Staff Need", breaks = brks.y2), labels = lbls.y, breaks = brks.y, name = "Range of Hourly Sales") #+ ylim(0,2000)
  #print(head(conf.df, 15))
  print(day.plot)
}

staffing.day.plot.dept3.transactions <- function(df, nickname.of.time.range, palette.color = "green.palette"){
  
  green.palette <- c("#238b45", "#74c476", "#bae4b3")
  red.palette <- c("#cb181d", "#fb6a4a", "#fcae91")
  purple.palette <- c("#6a51a3", "#9e9ac8", "#cbc9e2")
  orange.palette <- c("#d94701", "#fd8d3c", "#fdbe85")
  lavender.palette <- c("#ae017e", "#f768a1", "#fbb4b9")
  
  if(palette.color == "green.palette"){
    dark.color = green.palette[1]
    medium.color = green.palette[2]
    light.color = green.palette[3]
  }else if(palette.color == "red.palette"){
    dark.color = red.palette[1]
    medium.color = red.palette[2]
    light.color = red.palette[3]
  }else if(palette.color == "purple.palette"){
    dark.color = purple.palette[1]
    medium.color = purple.palette[2]
    light.color = purple.palette[3]
  }else if(palette.color == "orange.palette"){
    dark.color = orange.palette[1]
    medium.color = orange.palette[2]
    light.color = orange.palette[3]
  }else if(palette.color == "lavender.palette"){
    dark.color = lavender.palette[1]
    medium.color = lavender.palette[2]
    light.color = lavender.palette[3]
  }
  
  library(lubridate)
  library(dplyr)
  library(ggplot2)
  
  df <- filter(df, Register == "9")
  df <- df[!duplicated(df$Transaction),] # one row per transaction
  
  df.agg <- aggregate(Total.Sales ~ Date.Sold.Round + Hour + Weekday, df, length) # unit switched to length
  conf.df <- data.frame()
  
  for(i in 9:23){
    #print(i)
    i.hour.vec <- df.agg %>%
      filter(Hour == i) %>%
      select(Total.Sales)

    quantiles <- quantile(as.numeric(unlist(i.hour.vec)), probs = c(.5, .025, .975, .125, .875, .25, .75))
    
    quantiles.hour <- c(i, quantiles)
    conf.df <- rbind(conf.df, quantiles.hour)

  }
  colnames(conf.df) <- c("hour", "estimate", "low95", "high95", "low75", "high75", "low50", "high50")
  
  # add column corresponding to # of staff needed based on $400 -> 2, $600 -> 3, etc
  conf.df$staff.estimate <- (round(conf.df$high50/200))
  
  lbls = as.character(c(seq(9, 12, 1), seq(1, 11, 1)))#, c(rep("am", 3), rep("pm", 11)))
  brks <- seq(9,23, 1)
  
  lbls.y = paste0("$", seq(0, 2000, 200))
  brks.y = seq(0, 2000, 200)
  brks.y2 = seq(0,8,1)
  
  day.plot <- ggplot(data = conf.df, aes(x=hour)) +
    geom_ribbon(aes(ymin=low95, ymax=high95), fill=dark.color, alpha = .75)+
    geom_ribbon(aes(ymin=low75, ymax=high75), fill=medium.color, alpha = .75)+
    geom_ribbon(aes(ymin=low50, ymax=high50), fill=light.color, alpha = .75)+
    geom_ribbon(aes(ymin=estimate-.2, ymax=estimate+.2), fill="black") + ylab("Transactions Per Hour")
    #geom_point(aes(y=staff.estimate*200), color="#FFFF33", size = 10) #+
  #scale_y_continuous(labels = scales::dollar)  
  #geom_text(aes(y = staff.estimate*200, label = staff.estimate), color = "black", size = 5, fontface = "bold")
  day.plot <- day.plot + scale_x_continuous(labels = lbls, breaks = brks) + 
    ggtitle(nickname.of.time.range)
    #scale_y_continuous(sec.axis = sec_axis(~./200, name = "Estimate of Hourly Staff Need", breaks = brks.y2), labels = lbls.y, breaks = brks.y, name = "Range of Hourly Sales") #+ ylim(0,2000)
  #print(head(conf.df, 15))
  #print(day.plot)
  day.plot

}
```

## Profiles of Sales For Staffing

!["Days of 2017 sorted into 4 profiles for staffing"](splash.staffing.profiles.png)

These color categories were built on 2017 data, and the categories were still applicable in 2018 because, 2018 numbers largely followed the shape of 2017.

\newpage

# Sales Ranges of Profiles

The next plots show the range of sales that have occurred for each hour of the day.  The darkest, largest blue range represents where sales occur that hour 95% of the time.  The next slightly lighter blue range represents where hourly sales will fall 75%.  The smallest, lightest blue contains hourly sales 50% of the time.  The red ribbon is the average sales level for that hour.

In purple is a similar plot based on transactions by hour rather than sales dollars by hour.  The ranges for light purple, medium purple, and dark purple are the same as described above for the blues.


```{r staffing.kable, echo=FALSE}
##### Approximate Staffing Needs At Hourly Sales Levels

staffing.table <- data.frame("Sales Per Hour" = c("$400", "$600", "$800", "$1000", "$1200"), "Staff Needed" = c(2, 3, 4, 5, 6))

#kable(staffing.table)
```

Below are the plots of each profile:

\newpage

## #1 Jan-Apr - Grey
This blue plot is likely affected by a small sample size, but it does point to the fact that bigger purchases seemed to happen "first thing" but still only 1 transaction per hour at that time (based on purple plot)

```{r Off.Peak.Min, echo=FALSE, warning=FALSE, message=FALSE, fig.height= 4}

df.off.peak <- data2years.wide %>%
  filter(Date.Sold >= "2018-01-01", Date.Sold <= "2018-04-30")
#df2 <- data2years.wide %>%
#  filter(Date.Sold >= "2017-01-01", Date.Sold <= "2017-04-30")

#df.all <- rbind(df1, df2)
staffing.day.plot.dots(df.off.peak)
```


```{r,echo=FALSE, fig.height= 4}
staffing.day.plot.dept3.transactions(df.off.peak, "Jan-Apr Transactions", palette.color = "purple.palette")
```

\newpage

## #2 May, Jun, Sep - Green
If you were to add a second person or have people overlap, the 2-3pm or possibly 1-3pm time period is most likely to have more customers and more dollars.  The dark blue rising higher in the morning means that occasionally there are more sales and customers during that 10-1 period.

```{r near.peak, echo=FALSE, message=FALSE, warning=FALSE, fig.height= 3.8}

df1 <- data2years.wide %>%
  filter(Date.Sold >= "2018-05-01", Date.Sold <= "2018-06-30")
#df2 <- data2years.wide %>%
#  filter(Date.Sold >= "2017-05-01", Date.Sold <= "2017-06-30")
df3 <- data2years.wide %>%
  filter(Date.Sold >= "2018-09-01", Date.Sold <= "2018-09-30")
#df4 <- data2years.wide %>%
#  filter(Date.Sold >= "2017-09-01", Date.Sold <= "2017-09-30")

df.all <- rbind(df1, df3)
staffing.day.plot.dots(df.all)
```


```{r,echo=FALSE, fig.height= 3.8}
staffing.day.plot.dept3.transactions(df.all, "May, Jun, Sep Transactions", palette.color = "purple.palette")
```

\newpage

## #3 Jul-Aug - Blue
This period provides a similar picture where 2-3pm is the timeframe most likely to have higher sales and more transactions per hour.  Keep in mind that even during that time there are on average only 2 purchases per hour, but that wouldn't account for people browsing and not buying who also need to get their questions answered.


```{r Spring.Fri.Sat.Tan.Mid.ApriltoMay, echo=FALSE, warning=FALSE, fig.height= 3.8}

df.peak <- data2years.wide %>%
  filter(Date.Sold >= "2018-07-01", Date.Sold <= "2018-08-31")
#df2 <- data2years.wide %>%
#  filter(Date.Sold >= "2017-07-01", Date.Sold <= "2017-08-31")

#df.all <- rbind(df1, df2)
staffing.day.plot.dots(df.peak)
```


```{r,echo=FALSE, fig.height= 3.8}
staffing.day.plot.dept3.transactions(df.peak, "Jul-Aug Transactions", palette.color = "purple.palette")
```

\newpage

## #4 Oct-Dec - Pink
Sales are typically low during this period but at 6pm and 8pm there are sometimes increases. I don't know whether it would warrant additional staffing but those are the times it's most likely for Splash to be busy during this period.

```{r Spring.Sundays, echo=FALSE, warning=FALSE, fig.height= 3.8}

df.q4 <- data2years.wide %>%
  filter(Date.Sold >= "2018-10-01", Date.Sold <= "2018-12-31")
#df2 <- data2years.wide %>%
#  filter(Date.Sold >= "2017-10-01", Date.Sold <= "2017-12-31")

#df.all <- rbind(df1, df2)
staffing.day.plot.dots(df.q4)
```

```{r,echo=FALSE, fig.height= 3.8}

staffing.day.plot.dept3.transactions(df.q4, "Oct-Dec Transactions", palette.color = "purple.palette")
```



### Appendix: Details on calculation of more staff compared with more sales

#### Additional sales to be gained
For the peak and near peak periods, the difference between the top of the darkest blue area and the next darkest blue area, is about $600 for each hour, and it's about a 4 hour period where the dark blue is much higher than the next blue.  The difference between those areas is about a 10% chance. Next, note that these represent a full 60 minutes so it's possible that even on busy times customers could come one after another never simultaneously.  Even if they come at the same time, one employee could serve multiple customers to some extent.  For that reason, I'd calculate the upside as follows

$600 difference x 4 hours x 150 days (May-Sept) x 10% chance of it happening = $36,000, but this isn't the end. Since all the sales don't come at the same time and one worker can server more than one customer per hour, this number can by divided by 2 or 4 for what's still probably a generous upside of $9,000-$18,000

#### Additional compensation
For compensation, I used 150 days x 4 hours for 600 labor hours times pay rate for the pay.  That could be $7,200 to $9,000 or a wider range depending on the pay rate. If, instead of having the additional person for 4 years, it was only 2 hours from 1-3, the pay calculation would be cut in half.  The upside would be cut by a little less than half, because those 1-3 is more likely to have multiple customers than either 12-1 or 3-4.

