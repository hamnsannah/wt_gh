---
title: "Staffing From Sales - Splash"
author: "Allocate Analytics"
date: "February 18, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(ggplot2)
library(sugrrants)
library(lubridate)
library(fpp2)
#library(prophet)

#data.clean <- read.csv("data/mutated.data1417.csv", stringsAsFactors = FALSE)

#####
import.jh.sql.connect <- function(db.name.in.sql.server){
  library(odbc)
  library(DBI)
  
  con <- DBI::dbConnect(odbc::odbc(),
                        Driver = "SQL Server",
                        Server = "LAPTOP-JHRVTF04\\SQLEXPRESS",
                        Database = db.name.in.sql.server,
                        Trusted_Connection = "True")
  con
}

import.jh.detailed.sales <- function(){
  joined.df <- DBI::dbGetQuery(con,'
                                SELECT TransactionEntry.ItemID, TransactionEntry.Price, 
                                 TransactionEntry.TransactionTime, TransactionEntry.Quantity, 
                                  TransactionEntry.TransactionNumber, Item.Description, 
                                 Department.Name AS "Department", Category.Name AS "Category", 
                                 SupplierName AS "Supplier", Cashier.Name AS "CashierName", 
                                 Register.Number AS "Register", Register.Description AS "RegLocation"

                                 FROM TransactionEntry
                                 
                                 JOIN Item ON TransactionEntry.ItemID = Item.ID
                                 JOIN Department ON Item.DepartmentID = Department.ID
                                 JOIN Category ON Item.CategoryID = Category.ID
                                 JOIN Supplier ON Item.SupplierID = Supplier.ID
                                 JOIN [Transaction] ON TransactionEntry.TransactionNumber = [Transaction].TransactionNumber
                                 JOIN Batch ON [Transaction].BatchNumber = Batch.BatchNumber
                                 JOIN Register ON Batch.RegisterID = Register.ID
                                 JOIN Cashier ON [Transaction].CashierID = Cashier.ID')
  joined.df
  }

clean.jh.detailed.sales <- function(sql.df){
  library(dplyr)
  library(lubridate)
    
    sql.df <- sql.df %>%
      rename(Sold.Price = Price, Cashier = CashierName, Date.Sold = TransactionTime,
             Qty.Sold = Quantity, Transaction = TransactionNumber) %>%
      mutate(Total.Sales = Qty.Sold*Sold.Price) %>%
      select(Department, Category, Supplier, ItemID, Description, Qty.Sold, Sold.Price, 
             Total.Sales, Transaction, Date.Sold, Register, Cashier)
    
    sql.df$Date.Sold <- ymd_hms(sql.df$Date.Sold)
    sql.df$Year <- year(sql.df$Date.Sold)
    sql.df$Month <- month(sql.df$Date.Sold)
    sql.df$Department <- trimws(sql.df$Department)
    sql.df$Category <- trimws(sql.df$Category)
    sql.df$Supplier <- trimws(sql.df$Supplier)
    sql.df$Item <- trimws(sql.df$Item)
    #print(head(sql.df))
    sql.df$Dept.By.Year <- paste(sql.df$Department, sql.df$Year)
    sql.df$Categ.By.Year <- paste(sql.df$Category, sql.df$Year)
    sql.df
}
if(file.exists(paste0("data/sales.data.cache", Sys.Date(), ".csv"))){
  data.clean <- read.csv(paste0("data/sales.data.cache", Sys.Date(), ".csv"), stringsAsFactor = FALSE)
} else {
con <- import.jh.sql.connect("Sam's Store")

new.df <- import.jh.detailed.sales() # uses con but not as argument

data.clean <- clean.jh.detailed.sales(new.df)

write.csv(data.clean, paste0("data/sales.data.cache", Sys.Date(), ".csv"), row.names = FALSE)
}

#####
data.clean <- filter(data.clean, Register == "9") #### Only Splash

data.wide <- data.clean
data.wide$Date.Sold <- ymd_hms(data.wide$Date.Sold)
data.wide$Date.Sold.Round <- floor_date(data.wide$Date.Sold, unit = "hour")
data.wide$Date.Sold <- as_date(data.wide$Date.Sold)
data.wide$Date.Sold.Round <- as_datetime(data.wide$Date.Sold.Round)

data17 <- filter(data.wide, Year == 2017)
#print(str(data17))
print(head(data17))

data17.agg <- aggregate(Total.Sales ~ Date.Sold.Round + Date.Sold, data17, sum)
data17.agg <- data17.agg %>%
  mutate("Hour" = as.integer(hour(Date.Sold.Round))) %>%
  frame_calendar(x = Hour, y = Total.Sales, date = Date.Sold, calendar = "monthly")
data17.plot <- data17.agg %>%
  ggplot(aes(x = .Hour, y = .Total.Sales, group = Date.Sold)) + geom_line()

```

### Staffing Based on Sales in Recent Months

This chart is meant to answer the question how many staff are needed at each hour of the day.  The main assumption here is that more sales mean more customers present, which requires more staff to support them.  For this particular data set, we need more info on current staffing levels to complete the analysis such as least number of employees to keep the store open, what is the highest staffing level scheduled at busiest time of the year, etc.  As a result, the calculations for the yellow dots are placeholders.

The blue areas and red line are based on actual data and the axis on the left shows the scale of dollars.  The lightest blue area represents a range that sales fall within for that hour 50% of the time.  The slightly darker blue represents the 75% range, and the darkest blue represents 95%.  The red is the mean or average sales level for that hour for the selected time period. 

This plot indicates that 4pm is most often the busiest hour of the day over about the past six months.  The average (red line) is around $250 for that hour from 4-5pm.  50% of the time sales are between around $200-$350 (lightest blue).  75% of the time sales are between $140-$420 (slightly darker blue), and 95% of the time sales are between $50-$560.

```{r off.peak.minimum, echo=FALSE, warning=FALSE, message=FALSE}

data2years.wide <- filter(data.wide, Year %in% c(2017, 2018))

#data2years$Date.Sold <- as_date(data2years$Date.Sold)

data2years.wide <- data2years.wide %>%
  mutate("Hour" = hour(Date.Sold.Round)) %>%
  mutate("Weekday" = wday((Date.Sold.Round)))
    
# filter for recent months since July ##
df1 <- data2years.wide %>%
  filter(Date.Sold >= "2018-07-01", Date.Sold <= "2018-12-17")
#df2 <- data2years.wide %>%
#  filter(Date.Sold >= "2016-02-01", Date.Sold <= "2016-04-07", Weekday %in% c(1, 2, 3, 4, 5, 7))

#print(head(df2))
#print(tail(df2))

df.all <- df1
#df.all <- rbind(df1, df2)

# need to go back to original script and add additional start dates with defaults like 0 for min and 100000 for max
staffing.day.plot.2 <- function(df, nickname.of.time.range){
  
  library(lubridate)
  library(dplyr)
  library(ggplot2)
  
  df.agg <- aggregate(Total.Sales ~ Date.Sold.Round + Hour + Weekday, df.all, sum)
  conf.df <- data.frame()
  
  for(i in 9:23){
    print(i)
    i.hour.vec <- df.agg %>%
      filter(Hour == i) %>%
      select(Total.Sales)
    #i.hour.vec <- as.double(i.hour.vec)
    print(head(i.hour.vec))
    #i.estimate.95 <- t.test(i.hour.vec, conf.level = 0.95)$estimate
    #i.conf.int.95 <- t.test(i.hour.vec, conf.level = 0.95)$conf.int
    #i.conf.int.75 <- t.test(i.hour.vec, conf.level = 0.75)$conf.int
    #i.conf.int.50 <- t.test(i.hour.vec, conf.level = 0.50)$conf.int
    #three.conf.int <- c(i, i.estimate.95, i.conf.int.95, i.conf.int.75, i.conf.int.50)
    quantiles <- quantile(as.numeric(unlist(i.hour.vec)), probs = c(.5, .025, .975, .125, .875, .25, .75))
    print(class(quantiles))
    
    quantiles.hour <- c(i, quantiles)
    print(class(quantiles.hour))
    conf.df <- rbind(conf.df, quantiles.hour)
    
    #i.estimate.95 <- quantie(i.hour.vec, .5)
    #i.conf.int.95.high <- 
    #quantiles.hour <- c(i, quantile(i.hour.vec, c(.5, .025, .975, .125, .875, .25, .75)))
  }
    colnames(conf.df) <- c("hour", "estimate", "low95", "high95", "low75", "high75", "low50", "high50")
    
    day.plot <- ggplot(data = conf.df, aes(x=hour)) +
      geom_ribbon(aes(ymin=low95, ymax=high95), fill="#0571b0", alpha = .75)+
      geom_ribbon(aes(ymin=low75, ymax=high75), fill="#92c5de", alpha = .75)+
      geom_ribbon(aes(ymin=low50, ymax=high50), fill="#d1e5f0", alpha = .75)+
      geom_ribbon(aes(ymin=estimate-20, ymax=estimate+20), fill="#ef8a62")
    print(day.plot)
}

#staffing.day.plot.2(df.all)


### function with staffing plot with yellow dots
staffing.day.plot.dots <- function(df, nickname.of.time.range){
  
  library(lubridate)
  library(dplyr)
  library(ggplot2)
  
  df.agg <- aggregate(Total.Sales ~ Date.Sold.Round + Hour + Weekday, df.all, sum)
  conf.df <- data.frame()
  
  for(i in 8:23){
    #print(i)
    i.hour.vec <- df.agg %>%
      filter(Hour == i) %>%
      select(Total.Sales)
    #i.hour.vec <- as.double(i.hour.vec)
    #print(head(i.hour.vec))
    #i.estimate.95 <- t.test(i.hour.vec, conf.level = 0.95)$estimate
    #i.conf.int.95 <- t.test(i.hour.vec, conf.level = 0.95)$conf.int
    #i.conf.int.75 <- t.test(i.hour.vec, conf.level = 0.75)$conf.int
    #i.conf.int.50 <- t.test(i.hour.vec, conf.level = 0.50)$conf.int
    #three.conf.int <- c(i, i.estimate.95, i.conf.int.95, i.conf.int.75, i.conf.int.50)
    quantiles <- quantile(as.numeric(unlist(i.hour.vec)), probs = c(.5, .025, .975, .125, .875, .25, .75))
    #print(class(quantiles))
    
    quantiles.hour <- c(i, quantiles)
    #print(class(quantiles.hour))
    conf.df <- rbind(conf.df, quantiles.hour)
    
    #i.estimate.95 <- quantie(i.hour.vec, .5)
    #i.conf.int.95.high <- 
    #quantiles.hour <- c(i, quantile(i.hour.vec, c(.5, .025, .975, .125, .875, .25, .75)))
  }
  colnames(conf.df) <- c("hour", "estimate", "low95", "high95", "low75", "high75", "low50", "high50")
  
  # add column corresponding to # of staff needed based on $400 -> 2, $600 -> 3, etc
  conf.df$staff.estimate <- (round(conf.df$high50/100)) ###### should be /200 for Whale's Tale
  
  lbls = paste0(as.character(c(seq(8, 12, 1), seq(1, 11, 1))), c(rep("am", 4), rep("pm", 12)))
  brks <- seq(8,23, 1)
  
  lbls.y = paste0("$", seq(0, 2000, 200))
  brks.y = seq(0, 2000, 200)
  brks.y2 = seq(0,8,1)
  
  ########
  #####change all these 100s to 200s if doing Whale's Tale, really need a way to isolate this equation in order to make changes easily
  ########
  
  day.plot <- ggplot(data = conf.df, aes(x=hour)) +
    geom_ribbon(aes(ymin=low95, ymax=high95), fill="#0571b0", alpha = .75)+
    geom_ribbon(aes(ymin=low75, ymax=high75), fill="#92c5de", alpha = .75)+
    geom_ribbon(aes(ymin=low50, ymax=high50), fill="#d1e5f0", alpha = .75)+
    geom_ribbon(aes(ymin=estimate-5, ymax=estimate+5), fill="#ef8a62") +
    geom_point(aes(y=staff.estimate*100), color="#FFFF33", size = 10) +
    geom_text(aes(y = staff.estimate*100, label = staff.estimate), color = "black", size = 5, fontface = "bold")
  day.plot <- day.plot + scale_x_continuous(labels = lbls, breaks = brks) + 
    scale_y_continuous(sec.axis = sec_axis(~./100, name = "Estimate of Hourly Staff Need", breaks = brks.y2), labels = lbls.y, breaks = brks.y, name = "Range of Hourly Sales") + theme_dark() #+ ylim(0,2000)
  #print(head(conf.df, 15))
  print(day.plot)
}

```

```{r peak, echo=FALSE, warning=FALSE, message=FALSE}

#2019 filters: peak 2018-05-24 to 2018-09-12
#2018-09-23-13 to 2018-10-14 AND 2018-04-27 to 2018-05-23

# filter for near.peak ##
df.peak1 <- data2years.wide %>%
  filter(Date.Sold >= "2018-05-24", Date.Sold <= "2018-09-12", Weekday %in% c(1, 2, 3, 4, 5, 6, 7))
#df.near2 <- data2years.wide %>%
#  filter(Date.Sold >= "2018-09-23", Date.Sold <= "2018-09-03", Weekday %in% c(1, 2, 3, 4, 5, 6, 7))
df.peak <- df.peak1
#print(head(df2))
#print(tail(df2))

#df.all <- df1
#df.all <- rbind(df1, df2)
staffing.day.plot.dots(df.peak)

```

```{r near.peak, echo=FALSE, warning=FALSE, message=FALSE}

#2019 filters: peak 2018-05-24 to 2018-09-12
#2018-09-23-13 to 2018-10-14 AND 2018-04-27 to 2018-05-23

# filter for near.peak ##
df.near1 <- data2years.wide %>%
  filter(Date.Sold >= "2018-04-27", Date.Sold <= "2018-05-23", Weekday %in% c(1, 2, 3, 4, 5, 6, 7))
df.near2 <- data2years.wide %>%
  filter(Date.Sold >= "2018-09-23", Date.Sold <= "2018-09-03", Weekday %in% c(1, 2, 3, 4, 5, 6, 7))
df.near <- rbind(df.near1, df.near2)
#print(head(df2))
#print(tail(df2))

#df.all <- df1
#df.all <- rbind(df1, df2)
staffing.day.plot.dots(df.near)

```

Additional Notes:

* As shown in the Appendix below, the sales for this store don't exhibit a very seasonal pattern, so there's not a need to look at weekend vs. weekday or January vs. June because there aren't clear differences.  However, there has been a significant upward trend in 2018.  In light of that, I included data from July 1, 2018 to Dec 17, 2018, which is the latest date for which I have full data.  The red area shows the average sales level for each hour for every day during that period.

* This report isn't meant to suggest managers should schedule people for 1 hour shifts or follow these levels exactly.  This report shows how many staff are likely to be required at a particular hour of the day  Management is best positioned to know how best to use this analysis while still offering logical shifts to employees.

* The succeeding pages contain additional plots I used to visualize the data during the exploratory phase of the analysis.  These are more technical and can be ignored if they aren't useful.  First are calendar plots that show the shape of sales throughout each day for the past three years.  Next is a time series decomposition that breaks down the data into its seasonal, trend, and random components.  A time series decomposition with monthly data comes first (page 5) then with daily data (page 6).

\newpage
# Appendix: 
##Additional graphs used to determine how to group days together, determine seasonality, and delineate beginning and end of data set

## 2018 Calendar Plot

```{r, calendar.18.plot, echo=FALSE}
data18 <- filter(data.wide, Year == 2018)
#print(str(data17))
#print(head(data17))

data18.agg <- aggregate(Total.Sales ~ Date.Sold.Round + Date.Sold, data18, sum)
data18.agg <- data18.agg %>%
  mutate("Hour" = as.integer(hour(Date.Sold.Round))) %>%
  frame_calendar(x = Hour, y = Total.Sales, date = Date.Sold, calendar = "monthly")
data18.plot <- data18.agg %>%
  ggplot(aes(x = .Hour, y = .Total.Sales, group = Date.Sold)) + geom_line()
prettify(data18.plot)
```

\newpage
## 2017 Calendar Plot

```{r calendar.17.plot, echo=FALSE, warning=FALSE, message=FALSE}
#print(unique(data.clean$Register))
prettify(data17.plot)
```

\newpage
## 2016 Calendar Plot

```{r calendar.16.plot, echo=FALSE, warning=FALSE, message=FALSE}
data16 <- filter(data.wide, Year == 2016)

data16.agg <- aggregate(Total.Sales ~ Date.Sold.Round + Date.Sold, data16, sum)
data16.agg <- data16.agg %>%
  mutate("Hour" = as.integer(hour(Date.Sold.Round))) %>%
  frame_calendar(x = Hour, y = Total.Sales, date = Date.Sold, calendar = "monthly")
data16.plot <- data16.agg %>%
  ggplot(aes(x = .Hour, y = .Total.Sales, group = Date.Sold)) + geom_line()
prettify(data16.plot)

```

\newpage
### Evaluating Seasonality and Trend: Monthly Data

Notes: 

* The drop off on the right side of the plot is due to only having about half a month of data for Dec 2018 as of the creation of this report.

* 2018 was really a break out year for this business in sales and the trend line suggests that is likely to continue into 2019 to an extent.

```{r time_series, echo = FALSE, warning=FALSE, message=FALSE, fig.height=3.7}

exploratory.jh.time.series <- function(clean.df, freq = 365){
  require(lubridate)
  require(dplyr)
  clean.df <- arrange(clean.df, Date.Sold)
  clean.df$Day <- date(clean.df$Date.Sold) # df should have been arranged in cleaning step
  start.year <- year(clean.df$Day)[1]
  if(freq == 365){
    start.day <- date(head(clean.df$Day,1)) - floor_date(head(clean.df$Day,1), unit = "year")+1 # subtracts earliest day from first day of year plus one
    sales.agg <- aggregate(Total.Sales ~ Day, clean.df, sum)
    all.dates <- seq.Date(from = as.Date(min(sales.agg$Day, na.rm = TRUE)), to = as.Date(max(sales.agg$Day, na.rm = TRUE)),by = 1)
    sales.agg.all <- left_join(data.frame(Day = all.dates), sales.agg)
    sales.ts <- ts(sales.agg.all$Total.Sales, start = c(start.year, start.day), frequency = 365)
  }
  if(freq == 12){
    start.month <- month(head(clean.df$Month,1))
    sales.agg <- aggregate(Total.Sales ~ Month + Year, clean.df, sum)
    #print(head(sales.agg, 25))
    sales.agg.all <- left_join(sales.agg, data.frame(Month = 1:12))
    sales.ts <- ts(sales.agg.all$Total.Sales, start = c(start.year, start.month), frequency = 12)
  }
  sales.ts[is.na(sales.ts)] <- 0
  
  sales.ts
}

sales.ts <- exploratory.jh.time.series(data.clean, freq = 12)
autoplot(sales.ts)
sales.stl <- stl(sales.ts, s.window = 5)
autoplot(sales.stl)
```

\newpage
### Evaluating Seasonality and Trend: Daily Data

```{r time_series_daily, echo=FALSE, warning=FALSE, message=FALSE}
sales.ts <- exploratory.jh.time.series(data.clean)
autoplot(sales.ts)
sales.stl <- stl(sales.ts, s.window = 5)
autoplot(sales.stl)
```